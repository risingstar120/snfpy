.. _usage_ref:

Usage
=====

Brief example
---------------
A brief example for those who just want to get started: ::

    >>> import numpy as np
    >>> import snf

    # load data matrices and ground truth cluster labels
    >>> data_files, dist_metric = ['data1.csv', 'data2.csv'], 'euclidean'
    >>> true_labels = np.loadtxt('labels.csv')
    >>> data_tuples = [(np.loadtxt(fname), dist_metric) for fname in data_files]

    # construct subj x subj affinity matrices from data
    >>> affinities = []
    >>> for (data, metric) in data_tuples:
    >>>     affinities += [snf.make_affinity(data, metric=metric, K=20, mu=0.5)]

    # compute fused matrix via SNF
    >>> fused_aff = snf.SNF(affinities, K=params['K'])

    # get optimal cluster number ("eigengap" method) and cluster
    >>> n_clusters = snf.get_n_clusters(fused_aff)[0]
    >>> fused_labels = snf.spectral_labels(fused_aff, n_clusters=n_clusters)

    # compute normalized mutual information for clustering solutions against ground truth
    >>> all_labels = [true_labels, fused_labels]
    >>> for arr in affinities:
    >>>     all_labels += [snf.spectral_labels(arr, n_clusters=n_clusters)]
    >>> nmi = snf.nmi(all_labels)

    # compute silhouette score to assess goodness-of-fit for clustering
    >>> silhouette = snf.silhouette_score(fused_aff, fused_labels)

In-depth example
----------------
Using SNF is pretty straightforward. There are only a handful of commands that
you'll need, and the output (a subject x subject array) can easily be carried
forward to any number of analysis pipelines.

Nonetheless, for a standard scenario, this package comes bundled with two
datasets provided by the original authors of SNF which can be quite
illustrative.

First, we'll load in this data and create a list of data/distance metric tuples.
The data array should be (N x M), where N is the number of subjects and M are
features. The distance metric will be something like ``'euclidean'`` or
``'sqeuclidean'``. Check out the documentation for
``scipy.spatial.distance.cdist`` for appropriate arguments. ::

    >>> import numpy as np
    >>> import snf
    >>> data_files, dist_metric = ['data1.csv', 'data2.csv'], 'euclidean'
    >>> true_labels = np.loadtxt('labels.csv')
    >>> data_tuples = [(np.loadtxt(fname), dist_metric) for fname in data_files]

Once we have our (data, distance) tuples, we need to create affinity matrices.
Unlike distance matrices, a higher number in an affinity matrix indicates
increased similarity. Thus, the highest numbers should always be along the
diagonal, since subjects are always most similar to themselves!

To construct our affinity matrix, we'll use ``snf.make_affinity``, which
first constructs a distance matrix (using the provided distance metric) and
then converts this into an affinity matrix based on a given subject's
similarity to their ``K`` nearest neighbors. As such, we need to provide a few
hyperparameters: ``K`` and ``mu``. ``K`` determines the number of nearest
neighbors to consider when constructing the affinity matrix; ``mu`` is a
scaling factor that weights the affinity matrix. While the appropriate numbers
for these varies based on scenario, a good rule is that ``K`` should be around
``N / 10``, and ``mu`` should be in the range (0.2 - 0.7). ::

    >>> affinities = []
    >>> for (data, metric) in data_tuples:
    >>>     affinities += [snf.make_affinity(data, metric=metric, K=20, mu=0.5)]

Once we have our affinity arrays, we can run them through the SNF algorithm. We
need to carry forward our ``K`` hyperparameter to this algorithm, as well. ::

    >>> fused_aff = snf.SNF(affinities, K=params['K'])

The array output by SNF is a fused affinity matrix; that is, it represents
data from all the inputs. It's designed to be full rank, and can thus be
subjected to clustering and classification. We'll do the former, now, by
estimating the number of clusters in the data via the "eigengap" method and
using spectral clustering to group the subjects into those clusters.

By default, ``snf.get_n_clusters`` returns two values, which are the two most
optimal number of clusters. We'll use the first (best) for our clustering. ::

    >>> n_clusters = snf.get_n_clusters(fused_aff)[0]
    >>> fused_labels = snf.spectral_labels(fused_aff, n_clusters=n_clusters)

Now we can compare the clustering of our fused matrix to what would happen if
we had used the data from either of the original matrices, individually.

To do this, we first need to generate cluster labels from the individual
affinity matrices. ::

    >>> all_labels = [true_labels, fused_labels]
    >>> for arr in affinities:
    >>>     all_labels += [snf.spectral_labels(arr, n_clusters=n_clusters)]

Then, we'll calculate the normalized mutual information score (NMI) between the
labels generated by SNF and the ones we just obtained. ::

    >>> nmi = snf.nmi(all_labels)
    >>> print(np.triu(nmi))
    [[1.         0.62231525 0.57014042 0.43765575]
     [0.         1.         0.63253856 0.40447861]
     [0.         0.         1.         0.2910661 ]
     [0.         0.         0.         1.        ]]

The output array is symmteric, so we'll show only the upper triangle for now.
The values range from 0 to 1, where 0 indicates no overlap and 1 indicates a
perfect correspondence between the two sets of labels.

The entry in (0, 1) indicates that the fused array generated by SNF has pretty
substantial overlap with the "true" labels from the datasets. The entries in
(0, 2) and (0, 3) indicate the shared information from the individual (unfused)
data arrays (from ``data1.csv`` and ``data2.csv``) with the true labels.

While this example has the true labels to compare against, in unsupervised
clustering we would not have such information. In these instances, the NMI
cannot tell us that the fused array is *superior* to the individual data
arrays. Rather, it can only help distinguish how much data from each of the
individual arrays is contributing to the fused network.

We can also assess how well the clusters are defined using the silhouette
score. These values range from -1 to 1, where -1 indicates a poor clustering
solution and 1 indicates a fantastic solution. ::

    >>> silhouette = snf.silhouette_score(fused_aff, fused_labels)
    >>> print(f"Silhouette score for the fused matrix is: {silhouette:.2f}")
    Silhouette score for the SNF fused matrix is: 0.69

This indicates that the clustering solution for the data is pretty good! We
could try playing around with the hyperparameters to see if we can improve our
fit (being careful to do so in a way that won't overfit to your data!). It's
worth noting that the silhouette score here is slightly modified to deal with
the fact that we're working with affinity matrices, NOT distance matrices. See
the :ref:`API reference <api_ref>` for more information.
